{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Practical 6 POS Tagging.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**A.\tSentence Tokenization, Word Tokenization, Parts of Speech Tagging and chunking of User defined text**"
      ],
      "metadata": {
        "id": "i1E2-9SrIV2N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UaR6cFyIVUw",
        "outputId": "30c75c87-d09c-421b-e938-9e55ead33345"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk import tag\n",
        "from nltk import tokenize\n",
        "nltk.download('punkt')\n",
        "from nltk import tokenize\n",
        "from nltk import chunk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "para = \"Hello! My name is Beena Kapadia. Today you'll be learning NLTK.\"\n",
        "sents = tokenize.sent_tokenize(para)\n",
        "print(\"\\nsentence tokenization\\n===================\\n\",sents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmFbtIvZIxzd",
        "outputId": "5613ec0c-d8d9-46ec-c619-80e9aaa86f86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "sentence tokenization\n",
            "===================\n",
            " ['Hello!', 'My name is Beena Kapadia.', \"Today you'll be learning NLTK.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#word tokenization\n",
        "print(\"\\nword tokenization\\n======\\n\")\n",
        "for index in range(len(sents)):\n",
        "  words = tokenize.word_tokenize(sents[index])\n",
        "  print(words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRdqMUoXI8bK",
        "outputId": "9f24d38d-e835-4227-92bd-cfd23e6e636c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "word tokenization\n",
            "======\n",
            "\n",
            "['Hello', '!']\n",
            "['My', 'name', 'is', 'Beena', 'Kapadia', '.']\n",
            "['Today', 'you', \"'ll\", 'be', 'learning', 'NLTK', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#POS TAGGING\n",
        "tagged_words=[]\n",
        "for index in range(len(sents)):\n",
        "  tagged_words.append(tag.pos_tag(words))\n",
        "  print(\"\\nPOS Tagging\\n===========\\n\",tagged_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3KsCFrfJM47",
        "outputId": "5d8950c6-016a-4809-b295-f79f33303e8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "POS Tagging\n",
            "===========\n",
            " [[('Today', 'NN'), ('you', 'PRP'), (\"'ll\", 'MD'), ('be', 'VB'), ('learning', 'VBG'), ('NLTK', 'NNP'), ('.', '.')]]\n",
            "\n",
            "POS Tagging\n",
            "===========\n",
            " [[('Today', 'NN'), ('you', 'PRP'), (\"'ll\", 'MD'), ('be', 'VB'), ('learning', 'VBG'), ('NLTK', 'NNP'), ('.', '.')], [('Today', 'NN'), ('you', 'PRP'), (\"'ll\", 'MD'), ('be', 'VB'), ('learning', 'VBG'), ('NLTK', 'NNP'), ('.', '.')]]\n",
            "\n",
            "POS Tagging\n",
            "===========\n",
            " [[('Today', 'NN'), ('you', 'PRP'), (\"'ll\", 'MD'), ('be', 'VB'), ('learning', 'VBG'), ('NLTK', 'NNP'), ('.', '.')], [('Today', 'NN'), ('you', 'PRP'), (\"'ll\", 'MD'), ('be', 'VB'), ('learning', 'VBG'), ('NLTK', 'NNP'), ('.', '.')], [('Today', 'NN'), ('you', 'PRP'), (\"'ll\", 'MD'), ('be', 'VB'), ('learning', 'VBG'), ('NLTK', 'NNP'), ('.', '.')]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#POS TAGGING\n",
        "tagged_words=[]\n",
        "for index in range(len(sents)):\n",
        "  tagged_words.append(tag.pos_tag(words))\n",
        "  print(\"\\nPOS Tagging\\n===========\\n\",tagged_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xj0QTk6kJRCK",
        "outputId": "7357d34c-32d2-4f9c-8a43-238038a6a9e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "POS Tagging\n",
            "===========\n",
            " [[('Today', 'NN'), ('you', 'PRP'), (\"'ll\", 'MD'), ('be', 'VB'), ('learning', 'VBG'), ('NLTK', 'NNP'), ('.', '.')]]\n",
            "\n",
            "POS Tagging\n",
            "===========\n",
            " [[('Today', 'NN'), ('you', 'PRP'), (\"'ll\", 'MD'), ('be', 'VB'), ('learning', 'VBG'), ('NLTK', 'NNP'), ('.', '.')], [('Today', 'NN'), ('you', 'PRP'), (\"'ll\", 'MD'), ('be', 'VB'), ('learning', 'VBG'), ('NLTK', 'NNP'), ('.', '.')]]\n",
            "\n",
            "POS Tagging\n",
            "===========\n",
            " [[('Today', 'NN'), ('you', 'PRP'), (\"'ll\", 'MD'), ('be', 'VB'), ('learning', 'VBG'), ('NLTK', 'NNP'), ('.', '.')], [('Today', 'NN'), ('you', 'PRP'), (\"'ll\", 'MD'), ('be', 'VB'), ('learning', 'VBG'), ('NLTK', 'NNP'), ('.', '.')], [('Today', 'NN'), ('you', 'PRP'), (\"'ll\", 'MD'), ('be', 'VB'), ('learning', 'VBG'), ('NLTK', 'NNP'), ('.', '.')]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**B.\tNamed Entity recognition of user defined text**"
      ],
      "metadata": {
        "id": "eK9AVcQfJwYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "# Load English tokenizer, tagger, parser and NER\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "#Process whole documnet\n",
        "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
        "\"Google in 2007, few people outside of the company took him \"\n",
        "\"seriously. “I can tell you very senior CEOs of major American \"\n",
        "\"car companies would shake my hand and turn away because I wasn’t \"\n",
        "\"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
        "\"this week.\")\n",
        "doc = nlp(text)\n",
        "#Analyze syntax\n",
        "print(\"Noun phrase: \", [chunk.text for chunk in doc.noun_chunks])\n",
        "print(\"Verb\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzfB8QTVJ0kE",
        "outputId": "95ffd23f-d9a7-4e90-ab9c-9a0371934d71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Noun phrase:  ['Sebastian Thrun', 'self-driving cars', 'Google', 'few people', 'the company', 'him', 'I', 'you', 'very senior CEOs', 'major American car companies', 'my hand', 'I', 'Thrun', 'an interview', 'Recode']\n",
            "Verb ['start', 'work', 'drive', 'take', 'can', 'tell', 'would', 'shake', 'turn', 'talk', 'say']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**c) Named Entity recognition with diagram using NLTK corpus – treebank.**\n",
        "\n",
        "**Executed on Python IDLE**\n"
      ],
      "metadata": {
        "id": "wMP0gbitMfVJ"
      }
    }
  ]
}